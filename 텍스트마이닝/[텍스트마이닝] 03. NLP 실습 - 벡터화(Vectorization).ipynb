# **Vectorization**
1300여개의 Data Scientist job description 정보를 활용해 벡터화 진행

### **1. 필요한 lib import, DF load**

import re
import string

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import spacy
import io
from google.colab import files

uploaded = files.upload()

df=pd.read_csv(io.BytesIO(uploaded['Data_Scientist.csv']))

### **2-1. CountVectorizer 이용한 벡터화**

### 이곳에서 과제를 진행해 주세요 ### 

nlp = spacy.load("en_core_web_sm")

def tokenize(document):
    doc = nlp(document)
    # punctuations: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]

vect = CountVectorizer(stop_words='english',
                       tokenizer = tokenize,
                       max_features=30000)

dtm_vect = vect.fit_transform(df['description'])

dtm_vect = pd.DataFrame(dtm_vect.todense(), columns=vect.get_feature_names())
dtm_vect

### **3. 높은 빈도의 단어 시각화** 

sum_df = pd.DataFrame(dtm_vect.sum()).reset_index()

!pip install squarify

import squarify

sum_df_top5 = sum_df.sort_values(by=0, ascending=False)[:5]
fig, ax = plt.subplots(figsize=(20,10))
squarify.plot(sizes=sum_df_top5[0], label=sum_df_top5['index'], value=sum_df_top5[0], alpha=0.6, text_kwargs={'fontsize':15})

### **2-2.TfidVectorizer 이용한 벡터화**

tfidf = TfidfVectorizer(stop_words='english'
                        ,tokenizer=tokenize)

dtm = tfidf.fit_transform(df['description'])

dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())
dtm

### **4. 특정 문서와 가장 유사한 문서 추출**
이 실습에서는, 88번째 index값과 가장 유사한 5개의 문서 추출

from sklearn.neighbors import NearestNeighbors

nearest_neighb = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')
nearest_neighb.fit(dtm)
nearest_neighb_list = nearest_neighb.kneighbors([dtm.iloc[88]])[1][0]

for i in nearest_neighb_list:
  print(df.iloc[i])
